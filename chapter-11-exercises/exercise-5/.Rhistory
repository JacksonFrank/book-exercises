# Install the `"nycflights13"` package. Load (`library()`) the package.
# You'll also need to load `dplyr`
#install.packages("nycflights13")  # should be done already
library("nycflights13")
library("dplyr")
# What was the average departure delay in each month?
# Save this as a data frame `dep_delay_by_month`
# Hint: you'll have to perform a grouping operation then summarizing your data
View(nycflights13)
# What was the average departure delay in each month?
# Save this as a data frame `dep_delay_by_month`
# Hint: you'll have to perform a grouping operation then summarizing your data
View(airports)
# What was the average departure delay in each month?
# Save this as a data frame `dep_delay_by_month`
# Hint: you'll have to perform a grouping operation then summarizing your data
View(flights)
# What was the average departure delay in each month?
# Save this as a data frame `dep_delay_by_month`
# Hint: you'll have to perform a grouping operation then summarizing your data
dep_delay_by_month <- group_by(flights, format(as.Date(deadline), "%M"))
# What was the average departure delay in each month?
# Save this as a data frame `dep_delay_by_month`
# Hint: you'll have to perform a grouping operation then summarizing your data
dep_delay_by_month <- group_by(flights, format(as.Date(time_hour), "%M"))
View(dep_delay_by_month)
View(dep_delay_by_month)
# What was the average departure delay in each month?
# Save this as a data frame `dep_delay_by_month`
# Hint: you'll have to perform a grouping operation then summarizing your data
dep_delay_by_month <- group_by(flights, month)
# What was the average departure delay in each month?
# Save this as a data frame `dep_delay_by_month`
# Hint: you'll have to perform a grouping operation then summarizing your data
dep_delay_by_month <- group_by(flights, month) %>% summarise(avg_dep_delay = mean(dep_delay))
View(dep_delay_by_month)
View(dep_delay_by_month)
# What was the average departure delay in each month?
# Save this as a data frame `dep_delay_by_month`
# Hint: you'll have to perform a grouping operation then summarizing your data
dep_delay_by_month <- group_by(flights, month) %>% summarise(avg_dep_delay = mean(dep_delay, na.rm = TRUE))
View(dep_delay_by_month)
# Which month had the greatest average departure delay?
best_month <- arrange(dep_delay_by_month, -avg_dep_delay) %>% top_n(1) %>% pull(month)
# If your above data frame contains just two columns (e.g., "month", and "delay"
# in that order), you can create a scatterplot by passing that data frame to the
# `plot()` function
plot(dep_delay_by_month)
# To which destinations were the average arrival delays the highest?
# Hint: you'll have to perform a grouping operation then summarize your data
# You can use the `head()` function to view just the first few rows
dep_delay_by_dest <- group_by(flights, dest) %>% summarise(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
arrange(-avg_dep_delay) %>% head()
View(dep_delay_by_dest)
View(dep_delay_by_dest)
# You can look up these airports in the `airports` data frame!
View(airports)
# Create a dataframe of the average arrival delays for each _destination_, then
# use `left_join()` to join on the "airports" dataframe, which has the airport
# information
# Which airport had the largest average arrival delay?
dep_delay_by_dest <- group_by(flights, dest) %>% summarise(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
arrange(-avg_dep_delay)
# Install the `"nycflights13"` package. Load (`library()`) the package.
# You'll also need to load `dplyr`
#install.packages("nycflights13")  # should be done already
library("nycflights13")
library("dplyr")
# Create a dataframe of the average arrival delays for each _destination_, then
# use `left_join()` to join on the "airports" dataframe, which has the airport
# information
# Which airport had the largest average arrival delay?
dep_delay_by_dest <- group_by(flights, dest) %>% summarise(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
arrange(-avg_dep_delay)
# Create a dataframe of the average arrival delays for each _destination_, then
# use `left_join()` to join on the "airports" dataframe, which has the airport
# information
# Which airport had the largest average arrival delay?
arr_delay_by_dest <- group_by(flights, dest) %>% summarise(avg_dep_delay = mean(arr_delay, na.rm = TRUE)) %>%
arrange(-avg_dep_delay)
left_join(airports, arr_delay_by_dest)
View(arr_delay_by_dest)
View(arr_delay_by_dest)
View(airports)
View(airlines)
# Load the httr and jsonlite libraries for accessing data
# You can also load `dplyr` if you wish to use it
install.packages("httr")
library(httr)
install.packages("jsonlite")
library(jsonlite)
# Create a variable base_uri that stores the base URI (as a string) for the
# Github API (https://api.github.com)
base_uri <- GET('https://api.github.com')
# Create a variable base_uri that stores the base URI (as a string) for the
# Github API (https://api.github.com)
base_uri <- 'https://api.github.com'
# Send a GET request to this endpoint (the `base_uri` followed by the
# `org_resource` path). Print the response to show that your request worked.
# (The listed URI will also allow you to inspect the JSON in the browser easily).
response <- GET(paste0(base_uri, org_resource))
# Under the "Repositories" category of the API documentation, find the endpoint
# that will list _repos in an organization_. Then create a variable named
# `org_resource` that stores the endpoint for the `programming-for-data-science`
# organization repos (this is the _path_ to the resource of interest).
org_resource <- "/orgs/:programming-for-data-science/repos"
# Send a GET request to this endpoint (the `base_uri` followed by the
# `org_resource` path). Print the response to show that your request worked.
# (The listed URI will also allow you to inspect the JSON in the browser easily).
response <- GET(paste0(base_uri, org_resource))
# Extract the content of the response using the `content()` function, saving it
# in a variable.
extracted_response <- content(response)
# Under the "Repositories" category of the API documentation, find the endpoint
# that will list _repos in an organization_. Then create a variable named
# `org_resource` that stores the endpoint for the `programming-for-data-science`
# organization repos (this is the _path_ to the resource of interest).
org_resource <- "/orgs/programming-for-data-science/repos"
# Send a GET request to this endpoint (the `base_uri` followed by the
# `org_resource` path). Print the response to show that your request worked.
# (The listed URI will also allow you to inspect the JSON in the browser easily).
response <- GET(paste0(base_uri, org_resource))
# Extract the content of the response using the `content()` function, saving it
# in a variable.
extracted_response <- content(response)
# Convert the content variable from a JSON string into a data frame.
response_data <- fromJSON(extracted_response)
# Extract the content of the response using the `content()` function, saving it
# in a variable.
extracted_response <- content(response, "text")
# Convert the content variable from a JSON string into a data frame.
response_data <- fromJSON(extracted_response)
View(response_data)
View(response_data)
# How many (public) repositories does the organization have?
print(nrow(response_data))
# Search queries require a query parameter (for what to search for). Create a
# `query_params` list variable that specifies an appropriate key and value for
# the search term (you can search for anything you want!)
query_params <- list(q = 'data', sort = 'stars')
# Send a GET request to the `search_endpoint`--including your params list as the
# `query`. Print the response to show that your request worked.
search_response <- GET(paste0(base_uri, search_endpoint), query = query_params)
# Now a second query:
# Create a variable `search_endpoint` that stores the endpoint used to search
# for repositories. (Hint: look for a "Search" endpoint in the documentation).
search_endpoint <- '/search/repositories'
# Send a GET request to the `search_endpoint`--including your params list as the
# `query`. Print the response to show that your request worked.
search_response <- GET(paste0(base_uri, search_endpoint), query = query_params)
print(search_response)
library(dplyr)
# Extract the content of the response and convert it from a JSON string into a
# data frame.
search_df <- content(search_response, "text") %>% fromJSON()
View(search_df)
View(search_df)
View(search_df)
View(search_df)
View(search_df)
View(search_df)
# How many search repos did your search find? (Hint: check the list names to
# find an appropriate value).
search_repos <- search_df[['total_count']]
print(search_repos)
# What are the full names of the top 5 repos in the search results?
top_5_repos <- search_list[['items']] %>% top_n(5) %>% pull(name)
# Extract the content of the response and convert it from a JSON string into a
# data frame.
search_list <- content(search_response, "text") %>% fromJSON()
# What are the full names of the top 5 repos in the search results?
top_5_repos <- search_list[['items']] %>% top_n(5) %>% pull(name)
print(top_5_repos)
